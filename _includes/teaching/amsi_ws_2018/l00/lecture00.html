<div id="content">

<div class="panel panel-default notes-panel">
<div class="panel-heading">
<a href="#notes-section-setup" data-toggle="collapse" class="notes-toggle"><h3>Basic setup and conventions</h3>
</a>
</div>
<div class="panel-body" class="collapse" id="notes-section-setup"><div class="outline-text-2" id="text-setup">
<p>
We work with a Riemannian manifold \((M, g)\) with \(\nabla\) denoting the Levi-Civita connection.
</p>

<p>
Vector fields will be denoted \(X, Y, Z, W, \dots\). Most to the time we will abuse notation and write \(X, Y, Z, W, \dots\) for tangent <i>vectors</i>. We do this because many tensors, such as the curvature tensor are defined for vector fields, are tensorial so at a fixed point only depend on the value of the vector fields in question at that fixed point.
</p>

<p>
<b>In practice what this usually means</b> is that tensorial expression such as \(\operatorname{Rm}(X, Y) Z\), is a priori only defined on vector fields (by differentiating them which makes no sense for vectors) but is tensorial hence it's value at a point \(x\) depends only on \(X, Y, Z\) at the point \(x\). Thus if we extend tangent vectors \(X, Y, Z \in T_x M\) to vector fields defined in a neighbourhood of \(x\), the value of \(\operatorname{Rm}(X, Y) Z\) at \(x\) will be independent of the extension. Rather than continually saying "for an extension", we will just assume whenever necessary that such an extension has been made and denote it by \(X, Y, Z\). <b>At times there will be exceptions to this abuse and they will be clearly stated</b>.
</p>

<p>
One forms will be written \(\alpha, \beta, \dots\) and the same abuse of not specifying whether we are working at a point or with a field as with vectors will be followed.
</p>

<p>
For iterated derivatives, we will write \(\nabla_X \nabla_Y Z\) and at times if clarity seems particularly important, as \(\nabla_X (\nabla_Y Z)\). Note that many authors will write \(\nabla_X \nabla Y Z\) to mean the second covaraint derivative of \(Z\) in directions \(X, Y\). We will <b>not</b> do this and denote the second covariant derivative as \(\nabla^2_{X, Y} Z\) or \(\nabla^2 Z (X, Y)\). The two notions are related by
\[
\nabla^2 Z (X, Y) = \nabla_X (\nabla_Y Z) - \nabla Z (\nabla_X Y).
\]
More details are given in <a href="#second_derivative">Second Derivative</a>.
</p>

<p>
In particular, when working in coordinates, or more generally with respect to a local frame, often you will see \(\nabla_i \nabla_j Z\) to mean \(\nabla^2 Z (\partial_i, \partial_j)\). For the most part, we will not work with respect to any local frames, but if the need arises we will write \(\nabla^2_{ij} Z\) for \(\nabla^2 Z(\partial_i, \partial_j)\).
</p>
</div>

</div>
</div>

<div class="panel panel-default notes-panel">
<div class="panel-heading">
<a href="#notes-section-org66b274b" data-toggle="collapse" class="notes-toggle"><h3>Vector Bundles</h3>
</a>
</div>
<div class="panel-body" class="collapse" id="notes-section-org66b274b"><div class="outline-text-2" id="text-2">
<div class="defn">
<p>
A vector bundle is a triple \((E, \pi, M)\) with \(\pi : E \to M\) and such that there is an open cover \(U_i\) (local trivialisation) of \(M\) with diffeomorphisms
\[
\varphi_i : \pi^{-1} (U_i) \to U_i \times \mathbb{R}^k
\]
satisfying
\[
\pi_1 \circ \varphi_i = \pi
\]
where \(\pi_1 : U_i \times \mathbb{R}^k \to \mathbb{R}^k\) is the projection onto the first factor, and such that the transitions functions
\[
\varphi_i^{-1} \circ \varphi_j : U_i \cap U_j \times \mathbb{R}^k \to U_i \cap U_j \times \mathbb{R}^k
\]
are of the form
\[
(x, X) \in U_i \cap U_j \times \mathbb{R}^k \mapsto (x, \tau_{ij}(x) \cdot X)
\]
where \(\tau_{ij} : U_i \cap U_j \to \operatorname{GL}_k(\mathbb{R})\) is a smooth map.
</p>

</div>

<p>
Given \(x \in M\), the <i>fibre</i> \(E_x\) of a vector bundle over \(x\) is the set \(\pi^{-1} (\{x\})\). It has a vector space structure obtained by choosing a local trivialistion \(\varphi : \pi^{-1}(U) \to U \times \mathbb{R}^k\) with \(x \in U\) and for \(X, Y \in E_x\), \(\lambda \in \mathbb{R}\) defining
\[
X +_{\varphi} Y = \varphi^{-1} ((x, \pi_2 \circ \varphi(X) + \pi_2 \circ \varphi(Y)))
\]
and
\[
\lambda \cdot_{\varphi} X = \varphi^{-1}((x, \lambda \cdot \pi_2 \circ \varphi(X)))
\]
where the \(\varphi\) subscripts on the operations \(+, \cdot\) are to emphasise that these definitions depend on the local trivialisation \(\varphi\). The idea of course is to next show that the definition is independent of the choice of local trivialisation.
</p>

<p>
Let's just unpack the definition a little first: given \(X \in E_x\) we have \(\pi(X) = x\). Then \(\varphi(X) = (x, \bar{X})\) for a uniqune \(\bar{X} \in \mathbb{R}^k\). This follows since \(\varphi(X) \in U \times \mathbb{R}^k\) satisfies \(\pi_1 \circ \varphi (X) = \pi(X) = x\). Similarly for \(Y\). Then to add \(X\) and \(Y\) we note that \(\pi_2 \circ \varphi(X) = \bar{X}\) (likewise for \(Y\)) and we add \(\bar{X}, \bar{Y} \in \mathbb{R}^k\) to get the element
\[
(x, \bar{X} + \bar{Y}) \in U \times \mathbb{R}^k.
\]
Then take \(\varphi^{-1}\) of the result to get an element in \(E_x\). Similar considerations apply to \(\lambda \cdot_{\varphi} X\).
</p>

<p>
<b>Quick exercise</b>: Check that \(X + Y\) so defined is in fact in \(E_x\).
</p>

<p>
<b>Quick exercise</b>: Check the same for \(\lambda X\).
</p>

<p>
<b>Quick exercise</b>: Check that \(+_{\varphi}, \cdot_{\varphi}\) satisfy the axioms for a vector space. That is, \(+_{\varphi}\) gives \(E_x\) the structure of an Abelian group and \(\cdot_{\varphi}\) distributes over \(+_{\varphi}\) appropriately etc.
</p>

<p>
Now let's see the independence of the choice of local trivialisation.
</p>

<p>
<b>Using the definitions exercise</b>: Let \(\varphi : \pi^{-1}(U) \to U \times \mathbb{R}^k\) and \(\psi : \pi^{-1}(V) \to V \times \mathbb{R}^k\) be local trivialisations with \(x \in U\) and \(x \in V\). Show that
\[
\varphi(X +_{\psi} Y) = \varphi(X +_{\varphi} Y)
\]
and hence, since \(\varphi\) is a diffeomorphism,
\[
X +_{\psi} Y = X +_{\varphi} Y.
\]
<i>Hint</i>: Show that
\[
\varphi(X +_{\psi} Y) = (x, \tau(x) \cdot (\pi_2 \circ \psi(X)) + \tau(x) \cdot (\pi_2 \circ \psi(Y)))
\]
where \(\tau(x) = \varphi \circ \psi^{-1} (x)\) is the transition function from \(\psi\) to \(\varphi\) at \(x\); it is an invertible linear transformation. On the other hand,
\[
\varphi(X +_{\varphi} Y) = (x, \pi_2 \circ \varphi(X) + \pi_2 \circ \varphi(Y)).
\]
To finish you need to argue that
\[
\tau(x) \cdot (\pi_2 \circ \psi(X)) = \pi_2 \circ \varphi(X).
\]
</p>
</div>

</div>
</div>

<div class="panel panel-default notes-panel">
<div class="panel-heading">
<a href="#notes-section-org0022f55" data-toggle="collapse" class="notes-toggle"><h3>Tensors Bundles</h3>
</a>
</div>
<div class="panel-body" class="collapse" id="notes-section-org0022f55"><div class="outline-text-2" id="text-3">
<p>
The main vector bundles of interest to use are the <i>tangent bundle</i> \(\pi: TM \to M\), the <i>co-tangent bundle</i> \(\pi^{\ast} : T^{\ast} M \to M\) and various tensor products \(\pi^p_q : T^p_q M = \otimes^p T^{\ast} M \bigotimes \otimes^q T^M \to M\). These are all constructed from \(TM\). For example, to construct \(T^{\ast} M\), we may first take the set
\[
T^{\ast} M = \bigsqcup_{x \in M} (T_x M)^{\ast}
\]
with projection map \(\pi^{\ast}\) that takes \(\alpha \in T^{\ast} M\) to the unique \(x \in M\) such that \(\alpha \in (T_x M)^{\ast}\). That is, \((T_x M)^{\ast} = \operatorname{Hom} (T_x M \to \mathbb{R})\) is the dual space of \(T_x M\) consisting of linear maps \(T_x M \to \mathbb{R}\) and \(T^{\ast} M\) is the disjoint union of all of them over \(x \in M\).
</p>

<p>
The vector bundle structure is obtained by the vector bundle structure for \(TM\) as follows: take a local trivialistion \(\varphi : \pi^{-1} U \to U \times \mathbb{R}^n\) of \(TM\). For any \(\alpha \in (\pi^{\ast})^{-1} (U)\) (so that \(\pi^{\ast} \alpha \in U\)) and any \((X^1, \dots, X^n) \in \mathbb{R}^n\) define
\[
\bar{\alpha}_{\varphi} ((X^1, \cdots, X^n)) = \alpha(\varphi^{-1} (\pi^{\ast}(\alpha), (X^1, \dots, X^n))).
\]
In other words, letting \(x = \pi^{\ast} (\alpha)\), we take any \((X^1, \cdots, X^n) \in \mathbb{R}^n\) and map \((x, (X^1, \cdots, X^n))\) to \(T_x M\) by \(\varphi^{-1}\) and then apply \(\alpha\). Remember \(\alpha\) is a linear map \(T_x M \to \mathbb{R}\) so that this is well defined and produces a real number.
</p>

<p>
<b>Quick Exercise from the definitions</b>: Check that \(\bar{\alpha}_{\varphi}\) is a linear map \(\mathbb{R}^n \to \mathbb{R}\).
</p>

<p>
We can identify \(\bar{\alpha}_{\varphi}\) with the element \((\bar{\alpha}_{\varphi} ((1, 0, \dots, 0)), \dots, \bar{\alpha}_{\varphi} ((0, \dots, 0, 1))) \in \mathbb{R}^n\).
</p>

<p>
So now we have a map
\[
\varphi^{\ast} : \alpha \in (\pi^{\ast})^{-1} (U) \mapsto (\pi^{\ast}(\alpha), (\bar{\alpha}_{\varphi} ((1, 0, \dots, 0)), \dots, \bar{\alpha}_{\varphi} ((0, \dots, 0, 1))) \in U \times \mathbb{R}^k.
\]
We can of course do this for any trivialisation of \(TM\) and so we obtain a candidate family \(\{\varphi_i^{\ast}\}\) of trivialisations for \(T^{\ast} M\) from the trivialisations \(\{\varphi_i\}\) of \(TM\). Notice that we have \(\pi_1 \circ \varphi^{\ast} = \pi^{\ast} (\alpha)\). We just need to verify the form of the transition functions
\[
(\varphi_i^{\ast})^{-1} \circ \varphi_j^{\ast}.
\]
</p>

<p>
<b>Exercise from the definitions and linear algebra</b>: Show that
\[
(\varphi_i^{\ast})^{-1} \circ \varphi_j^{\ast} (x, \bar{\alpha}_{\varphi}) = (x, (\tau_{ij}^{-1})^T(x) \cdot \bar{\alpha}_{\varphi})
\]
where \(\tau_{ij}\) is the transition function for \(TM\) and \(T\) denotes the transpose. <i>Hint</i>: you only really need to do this at the vector space level proving that an isomorphismr \(\tau : \mathbb{R}^n \to \mathbb{R}^n\) induces the isomorphism \((\tau^{-1})^T : (\mathbb{R}^n)^{\ast} \to (\mathbb{R}^n)^{\ast}\). Then the transition functions are obtained by just carrying the \(x\) through.
</p>

<p>
An equivalent way to construct to \(T^{\ast} M\) is to apply the so-called <i>vector bundle gluing lemma</i> to the transition functions \(\tau_{ij}^{\ast} = (\tau_{ij}^{-1})^T\). This approach gives a quite general approach to constructing new vector bundles from old; one just has to construct the appropriate local trivialisations from the existing ones. All the tensor bundles may be constructed this way for example.
</p>

<p>
<b>Exercise</b>: Look up the vector bundle gluing lemma.
</p>

<p>
Similar constructions apply to create the tensor bundles \(T^p_q M\). For example, the transition functions for \(T^1_1 M = T^{\ast}M \otimes TM\) are
\[
(\tau^1_1)_{ij} = \tau_{ij}^{\ast} \otimes \tau_{ij}.
\]
</p>

<p>
More generally,
\[
(\tau^p_q)_{ij} = \otimes^p \tau_{ij}^{\ast} \bigotimes \otimes^q \tau_{ij}.
\]
</p>
</div>

</div>
</div>

<div class="panel panel-default notes-panel">
<div class="panel-heading">
<a href="#notes-section-curvature_tensor" data-toggle="collapse" class="notes-toggle"><h3>The Curvature Tensor</h3>
</a>
</div>
<div class="panel-body" class="collapse" id="notes-section-curvature_tensor"><div class="outline-text-2" id="text-curvature_tensor">
<p>
Let \(X, Y, Z\) be vector fields. Define a new vector field by
\[
\operatorname{Rm}(X, Y) Z = \nabla_X \nabla_Y Z - \nabla_Y \nabla_X Z - \nabla_{[X, Y]} Z.
\]
</p>

<p>
Notice that \(\nabla_X \nabla_Y Z\) will include the variation of \(Y\) along \(X\) - namely \(\nabla_X Y\). This is undesirable since we want to measure the curvature of the space itself at each point using \(\operatorname{Rm}\), and this should not depend on how any particular vector field varies. Likewise for \(\nabla_Y \nabla_X Z\). The term \(\nabla_{[X, Y]} Z\) compensates precisely for this undesirable effect.
</p>

<p>
Another way of expressing this compensation is to say that \(\operatorname{Rm}\) is <i>tensorial</i> in \(X, Y\) so that for any smooth function \(f \in C^{\infty} (M)\) we have
\[
\operatorname{Rm}(fX, Y) Z = f \operatorname{Rm}(X, Y) Z = \operatorname{Rm}(X, fY) Z.
\]
</p>

<p>
<b><b>Exercise</b></b>: Using the Leibniz rule for the connection \(\nabla\) and the corresponding rule for the Lie bracket, prove the claimed tensorality in \(X, Y\).
</p>

<p>
As a consequence, although as written, \(\operatorname{Rm}\) is defined for vector <i>fields</i>, tensorality induces a well defined map defined on tangent vectors. As mentioned in <a href="#setup">Basic setup and conventions</a>, we will typically not differentiate by vector fields and tangent vectors when dealing with tensorial equations. But <i>just this time</i>, let us be very explicit: Let \(X, Y, Z \in T_x M\) be tangent vectors, let \(\bar{X}, \bar{Y}, \bar{Z}\) and \(\tilde{X}, \tilde{Y}, \tilde{Z}\) be vector fields defined on a neighbourhood of \(x\) such that
\[
\bar{X} (x) = X, \bar{Y} (x) = Y, \bar{Z} (x) = Z
\]
\[
\tilde{X} (x) = X, \tilde{Y} (x) = Y, \tilde{Z} (x) = Z.
\]
Then tensorality implies that
\[
\left(\operatorname{Rm}(\bar{X}, \bar{Y}) \bar{Z}\right) (x) = \left(\operatorname{Rm}(\tilde{X}, \tilde{Y}) \tilde{Z}\right) (x).
\]
Thus we may define unambiguously,
\[
\operatorname{Rm}(X, Y) Z = \left(\operatorname{Rm}(\bar{X}, \bar{Y}) \bar{Z}\right) (x)
\]
where \(\bar{\cdot}\) denotes any arbitrary extension of \(X, Y, Z\). Tensorality then guarantees the result is independent of the extension.
</p>

<p>
What is rather more suprising, given that \(X\) is being differentiated twice, is that \(\operatorname{Rm}\) is tensorial in \(Z\) also! This means that \(\operatorname{Rm}\) may be evaluated on tangent vectors \(X, Y, Z\) at a point and thus may be interpreted as giving information (via \(\nabla\) which itself is determined by \(g\)) about \((M, g)\) at a point. This information is in fact a measure of curvature.
</p>

<p>
One question stands out: <b><b>Why is \(\nabla_{[X, Y]} Z\) the right correction term?</b></b> There are a few ways we might answer this question such as "because it works!" and "check in coordinates". The answer we will give here is obtained by interpreting \(\operatorname{Rm}\) as the <i>commutator of second derivatives</i>.
</p>
</div>

</div>
</div>

<div class="panel panel-default notes-panel">
<div class="panel-heading">
<a href="#notes-section-second_derivative" data-toggle="collapse" class="notes-toggle"><h3>Second Derivative</h3>
</a>
</div>
<div class="panel-body" class="collapse" id="notes-section-second_derivative"><div class="outline-text-2" id="text-second_derivative">
<p>
The second derivative of a vector field, in directions \(X, Y\) is defined to be
\[
\nabla^2_{X, Y} Z := \nabla_X (\nabla_Y Z) - \nabla Z (\nabla_X Y) = \nabla_X (\nabla_Y Z) - \nabla_{\nabla_X Y} Z.
\]
</p>

<p>
<b><b>Exercise</b></b>: Check that \(\nabla^2_{X, Y} Z\) is tensorial in \(X, Y\).
</p>

<p>
The reason for this definition is that once again, \(\nabla_X (\nabla_Y Z)\) will include the variation, \(\nabla_X Y\) of \(Y\) along \(X\) so we must subtract it off so that it doesn't contribute to \(\nabla^2 Z\). Essentially the way to understand how to choose what to substract off is by the product rule. First, for those more comfortable with coordinates, we have
\[
\nabla_Y Z = Y^i \partial_i Z^j \partial_j + Y^i Z^j \Gamma_{ij}^k \partial_k.
\]
This looks pretty good: we are differentiating \(Z\) in the direction \(Y\) and the result depends only on \(Y\), \(Z\) and the first derivatives of \(Z\). Now we apply \(\nabla_X\):
\[
\nabla_X \nabla_Y Z = X^{l} \partial_{l} (Y^i \partial_i Z^j) \partial_j + X^{l} Y^i \partial_i Z^j \Gamma^m_{l j} \partial_m + \cdots
\]
where I got tired of computing this way to I just put \(\cdots\) to indicate there are more terms! The point though is that there are derivatives of \(Y^i\) in there but we really only want to compute the variation of \(Z\). In particular notice that applying the product rule will give a term
\[
X^{l} \partial_{l} Y^i \partial_i Z^j \partial_j
\]
which we recognise as the first term occuring in
\[
\nabla_{\nabla_X Y} Z = X^{l} \partial_{l} Y^i \partial_i Z^j \partial_j + \cdots
\]
</p>

<p>
If one is so inclined, this computation may be fully carried out to verify that the result only depends on the components \(X^i, Y^j, Z^k\) and the first two derivatives of \(Z\): \(\partial_i Z^k, \partial_i \partial_j Z^k\). It's worth doing and doesn't actually take very long. Doing is better than reading, hence we have:
</p>

<p>
<b><b>Exercise</b></b>: Carry out the computation if you are so inclined.
</p>
</div>

</div>
</div>

<div class="panel panel-default notes-panel">
<div class="panel-heading">
<a href="#notes-section-hessian" data-toggle="collapse" class="notes-toggle"><h3>The Hessian of a function</h3>
</a>
</div>
<div class="panel-body" class="collapse" id="notes-section-hessian"><div class="outline-text-2" id="text-hessian">
<p>
For comparsion, consider the hessian matrix of a real valued function defined on \(\mathbb{R}^n\):
\[
d^2 f (x) = \begin{pmatrix}
\frac{\partial^2 f}{\partial x^1 \partial x^1} (x) & \cdots & \frac{\partial^2 f}{\partial x^1 \partial x^n} (x) \\
\vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x^n \partial x^1} (x) & \cdots & \frac{\partial^2 f}{\partial x^n \partial x^n} (x)
\end{pmatrix}
\]
</p>

<p>
This matrix records how \(f\) varies to second order at \(x\). Once this matrix has been computed, second derivatives of \(f\) in directions \(X = (X^1, \dots, X^n)\) and \(Y = (Y^1, \dots, Y^n)\) may be computed as
\[
d^2 f (X, Y) = Y^T d^2 f X.
\]
However, if \(X, Y\) are vector fields, then in general,
\[
d^2 f \ne \partial_X (\partial_Y f)
\]
where
\[
\partial_X f = df(X)
\]
or equivalently \(\partial_X f = X(f)\) with \(X\) acting as a derivation. The problem is of course again the fact that \(Y\) will also be differentiated:
\[
\partial_X (\partial_Y f) = X^i \partial_i (Y^j \partial_j f) = X^i Y^j \partial_i \partial_j f + X^i \partial_i Y^j \partial_j f = d^2f (X, Y) + df(D_X Y)
\]
so that
\[
d^2 f (X, Y) = \partial_X (\partial_Y f) - df(D_X Y) = \partial_X (\partial_Y f) - \partial_{D_X Y} f.
\]
Now the point of tensorality is that just from the matrices for \(d^2 f\) and \(df\) at a point \(x\), the second derivative \(\partial_X (\partial_Y f)\) at \(x\) may be computed by linear algegra alone (i.e. matrix multiplication) with no further differentation required. <i>This is because of tensorality: \(d^2 f(X, Y)\) only depends on the value of \(X, Y\) at the point \(x\) and not in a neighbourhood</i>. In other words, we may pre-compute the matrices \(df\) and \(d^2 f\) once and for all, then apply them to any vectors to compute first and second derivatives. We may also approximate \(f\) to second order at any point without needing to compute any more derivatives.
</p>

<p>
As a simple comparison, this idea is essentialy used by a calculator (or computer) to compute \(\sin, \cos, \exp\) etc. The Taylor series is calculated once and for all (giving an expression for the coefficients that can be calculate easily or by storing in a table sufficiently many of the coefficients) and then hard wired into the calculator. Further calculation is by elementary artihmetric operators.
</p>

<p>
Thus the moral is to compute the maps \(x \mapsto df(x)\) and \(x \mapsto d^2f (x)\) from which any second derivatives may be later computed using linear algebra. This only works by using the tensorial first and second derivatives so we may later work pointwise!
</p>
</div>

</div>
</div>

<div class="panel panel-default notes-panel">
<div class="panel-heading">
<a href="#notes-section-hessian_tensorality" data-toggle="collapse" class="notes-toggle"><h3>Tensoriality of second derivatives</h3>
</a>
</div>
<div class="panel-body" class="collapse" id="notes-section-hessian_tensorality"><div class="outline-text-2" id="text-hessian_tensorality">
<p>
Now the definition of \(d^2 f\) should be compared immediately with the definition of \(\nabla^2 Z\). Formally, it is the same thing just with \(f\) replaced by \(Z\) and \(D\) replaced by \(\nabla\). This is suggestive that we have the correct expression for \(\nabla^2 Z\).
</p>

<p>
Let us know rephrase the expression for \(\nabla^2 Z\) and see how the tensorality arises.
</p>

<p>
The first observation is that \(\nabla Z\) is an endomorphism of \(TM\). That is an element of
\[
\operatorname{Hom}(TM, TM) \simeq T^{\ast} M \otimes TM.
\]
Then we may interpret \(\nabla Z (X) = \nabla_X Z\) in terms of contractions (traces) and tensor products:
\[
\nabla Z (X) = \operatorname{Tr} \nabla Z \otimes X
\]
where the trace is taken by contractinng the \(T^{\ast} M\) part of \(\nabla Z\) with \(X\). Notice in particular for so-called <i>indecomposable</i> elements of \(T^{\ast} M \otimes T^M\), namely those of the form \(\alpha \otimes X\) with \(\alpha\) a one-form we have
\[
\operatorname{Tr} \alpha \otimes X = \alpha(X).
\]
Now we'd like to be able to differentiate \(\alpha\). As before, if we differentiate the function \(\alpha(X)\) we will pick up derivatives of both \(\alpha\) and \(X\). So to isolate the derivative of \(\alpha\) we could subtract off the derivative of \(X\). Then we make the definition
\[
\nabla \alpha (X, Y) = \partial_X (\alpha(Y)) - \alpha(\nabla_X Y).
\]
</p>

<p>
<b><b>Exercise</b></b>: Check this is tensorial in \(X\) and \(Y\).
</p>

<p>
In terms of tensor products and traces we may express the defintion as
\[
\partial_X (\alpha(Y)) = \partial_X \operatorname{Tr} (\alpha \otimes Y) = \operatorname{Tr} (\nabla_X \alpha) \otimes Y + \operatorname{Tr} \alpha \otimes \nabla_X Y = \nabla_X \alpha (Y) + \alpha(\nabla_X Y).
\]
</p>

<p>
<i>Given a connection \(\nabla\) on \(TM\) and the (uniquely determined by identifying vector fields with derivations) connection on \(M \times \mathbb{R}\), we may define a unique connection on \(T^{\ast}M\) by requiring that the resulting three connections commute with traces and satisfy the Leibniz rule for the tensor product.</i>
</p>

<p>
Now how do we differentiate \(\nabla Z\)? It is an endomorphism and we may do something similar for endomorphisms. So let \(T\) be and endomorphism so that \(T(X)\) is a vector field. Note that for one-forms \(\alpha\) we had \(\alpha(X)\) is a function and we know how to differentiate functions. Well, given \(\nabla\) we also know how to differentiate vector fields suggesting that we define
\[
(\nabla_X T) (Y) = \nabla_X (T(Y)) - T(\nabla_X Y).
\]
In terms of traces
\[
\nabla_X (T(Y)) = \nabla_X (\operatorname{Tr} T \otimes Y) = \operatorname{Tr} \nabla_X T \otimes Y + \operatorname{Tr} T \otimes \nabla_X Y = \nabla_X T (Y) + T(\nabla_X Y).
\]
Rearranging gives
\[
(\nabla_X T) (Y) = \nabla_X (T(Y)) - T(\nabla_X Y).
\]
</p>

<p>
<b><b>Exercise</b></b>: Check directly that this is tensorial in both \(X\) and \(Y\). Do it both with the final expression and with the identities using traces and tensor products. Think about how requiring that the connection commutes with traces and satisfies the Leibniz product rule for tensor products leads to tensorality.
</p>

<p>
Then for \(T = \nabla Z\) we finally obtain
\[
\nabla^2_{X, Y} Z = \nabla^2 Z (X, Y) = (\nabla_X \nabla Z) (Y) = \nabla_X (\nabla Z(Y)) - \nabla Z(\nabla_X Y) = \nabla_X \nabla_Y Z - \nabla_{\nabla_X Y} Z
\]
which is tensorial in both \(X\) and \(Y\).
</p>
</div>

</div>
</div>

<div class="panel panel-default notes-panel">
<div class="panel-heading">
<a href="#notes-section-ricci_identities" data-toggle="collapse" class="notes-toggle"><h3>Ricci Identities and tensorality of second derivatives</h3>
</a>
</div>
<div class="panel-body" class="collapse" id="notes-section-ricci_identities"><div class="outline-text-2" id="text-ricci_identities">
<p>
Now that we understand second derivatives, we can express the curvature tensor \(\operatorname{Rm}\) as the commutator of second derivatives:
\[
\operatorname{Rm} (X, Y) Z = \nabla^2_{X, Y} Z - \nabla^2_{Y, X} Z.
\]
This equation is known as the <i>Ricci Identity</i>.
</p>

<p>
<b><b>Exercise</b></b>: Prove the Ricci Identity. <i>Hint</i>: Use the fact that \(\nabla\) is torsion-free \(\nabla_X Y - \nabla_Y X = [X, Y].\)
</p>

<p>
Sometimes this expression is written
\[
[\nabla_X, \nabla_Y] Z = \nabla^2_{X, Y} Z - \nabla^2_{Y, X} Z.
\]
Be <i>careful</i> with this phrasing: \([\nabla_X, \nabla_Y] Z \ne \nabla_X (\nabla_Y Z) - \nabla_Y (\nabla_X Z)\)! The right hand side is not tensorial.
</p>


<p>
<b><b>Exercise</b></b>: Define \(\operatorname{Rm}(X, Y)f = \nabla^2_{X, Y} f - \nabla^2_{Y, X} f\). Show that \(\operatorname{Rm} (X, Y) f = 0\). Equivalently, \(\nabla^2 f(X, Y) = \nabla^2 f(Y, X)\). We might then say that \(M \times \mathbb{R} \to M\) is a flat (i.e. not curved!) vector bundle.
</p>

<p>
Thus the curvature tensor measures the lack of commutativity of second derivatives of vector fields. Put another way, unlike for functions, \(\nabla^2_{X, Y} Z\) need not be symmetric. Instead we have
\[
\nabla^2_{X, Y} Z = \nabla^2_{Y, X} Z + \operatorname{Rm} (X, Y) Z.
\]
</p>

<p>
<b><b>Exercise</b></b>: Show that in Euclidean space, \(\nabla^2_{X, Y} Z\) is symmetric in \(X, Y\).
</p>

<p>
Now we observe that since we defined \(\nabla^2 Z\) in a tensorial way, immediately we have \(\operatorname{Rm}(X, Y)Z\) is tensorial in \(X, Y\). By defining \(\operatorname{Rm}\) as the second order commutator, we also immediately obtained the correction term.
</p>

<p>
But still, we have the question <b><b>why is \(\operatorname{Rm}\) tensorial in \(Z\)?</b></b>
</p>

<p>
<b><b>Exercise</b></b> Show that \(\nabla_X \nabla_Y fZ - \nabla_Y \nabla_X fZ - \nabla_{[X,Y]} fZ = f \operatorname{Rm} (X, Y) Z + (\operatorname{Rm} (X, Y) f) Z = f \operatorname{Rm} (X, Y) Z.\) Thus we conclude the tensorality in \(Z\) follows since \(M \times \mathbb{R} \to M\) is a flat vector bundle.
</p>
</div>

</div>
</div>
</div>
